Introduction:
Cryptography is the art of writing messages in code or cipher, to disguise, and thereby secure the content of a particular stream of text. When encrypted, a plain text message can be revealed only through the use of the key used to encode the cipher. Cryptography does not mask the existence of the message, but does disguise its content [1]. In contrary, cryptanalysis is the art of recovering the plaintext of a message without access to the key. Successful cryptanalysis may recover the plaintext or the key for a specific ciphertext [2].

There are five general types of cryptanalytic attacks:-
1. Ciphertext-only attack: In this type of attack, the cryptanalyst has a series of cipher texts encrypted using the same encryption algorithm. Then, the cryptanalyst deduces the plain text of each of the cipher texts or identifies the key used to encrypt the cipher text

2. Known-plaintext attack: In this type of attack, the cryptanalyst has a series of ciphertext and their corresponding plaintext values encrypted using a specific key. The cryptanalyst then tries to deduce the key by forming a relationship between the ciphertext and plaintext entries.

3. Chosen-plaintext attack: In this type of attack, the cryptanalyst not only has access to the ciphertext and associated plaintext for several messages, but he also chooses the plaintext that gets encrypted. His job is to deduce the key used to encrypt the messages or an algorithm to decrypt any new messages encrypted with the same key.

4. Frequency analysis: It is the study of thefrequency of lettersor groups of letters in aciphertext. The method is used as an aid to breakingclassical ciphers. Frequency analysis is based on the fact that, in any given stretch of written language, certain letters and combinations of letters occur with varying frequencies.

5. Rubber-hose cryptanalysis: The cryptanalyst threatens, tortures or blackmails the person who has the key until they give it up.

Among the many cryptanalytic techniques, frequency analysis or frequency counting is the most basic technique applied to break substitution cipher based algorithms, among the varied list of attack techniques. The basic use of frequency analysis is to first count the frequency of ciphertext letters and then associate guessed plaintext letters with them. More complex use of statistics can be conceived, such as considering counts of pairs of letters digrams, trigrams, and so on. This is done to provide more information to the cryptanalyst.

It exploits the weakness in the substitution cipher algorithm to encrypt similar plaintext letters to similar ciphertext letters. Frequency analysis based cryptanalysis techniques were used to break ciphers based on the traditional cryptographic algorithms, but they do not work well with the modern block cipher based cryptographic algorithms.

Statistical properties of English:
Frequency analysis based cryptanalysis uses the fact that natural language is not random in nature and single alphabetic based substitution does not hide the statistical properties of the natural language. In the case of encryption using monoalphabetic substitution, to start deciphering the encryption it is useful to get a frequency count of all the letters. The most frequent letter may represent the most common letter in English, E followed by T, A, O and I whereas the least frequent are Q, Z and X [7]. Statistical patterns in a language can be detected by tracing the redundancy of the text in the language. It has been realized that various universal regularities characterize text from different domains and languages. The best-known is Zipf's law on the distribution of word frequencies [5], according to which the frequency of terms in a collection decreases inversely to the rank of the terms. Zipf's law has been found to apply to collections of written documents in virtually all languages [5].

English language characters have a very high redundancy rate when used for cryptographic substitutions. If we have a message encrypted using the substitution cipher that needs to be cracked, we can use frequency analysis. "In other words, if the sender has used an encryption scheme, that replaces one letter in the English to be another letter in English, we can still recognize the original plain text as, the frequency characteristics of the original plain text will be passed on the new cipher text characters "[4]. To apply frequency analysis, we will need to know the frequency of every letter in the English alphabet, or the frequency characteristics of the language used by the sender to encrypt the text.

Below is a list of average frequencies for letters in the English language. So, for example, the letter E accounts for 12.7% of all letters in English, whereas Z accounts for 0.1 %. All the frequencies are tabulated and plotted below:-

For example, let us consider the following sentence: "We study Cryptography as part of our course". Using a simple substitution cipher, let us consider the following:

So, the cipher text becomes: "yg uvwfa etarvqitcrja cu rctv qh qwt eqwtug". A simple frequency analysis of the cipher text can be carried out and the results are as given below:

The above data can be used by a cryptanalyst to identify the key or the plaintext by using simple substitution to the cipher text till a suitable plaintext value is not identified.

Apart from the use of mono alphabetic frequency analysis, cryptanalysts also identify frequency of paired letters better known as digram frequency and that of three letter words, called as Trigram frequencies. These help the cryptanalyst to exploit the redundant features of English language to break the cipher.

The most common Digrams (in order):

th, he, in, en, nt, re, er, an, ti, es, on, at, se, nd, or, ar, al, te, co, de, to, ra, et, ed, it, sa, em, ro.

The most common Trigrams (in order):

the, and, tha, ent, ing, ion, tio, for, nde, has, nce, edt, tis, oft, sth, men

Table 1: Digram and Trigram Frequencies [6]
These help in identifying the most commonly used terms in English to break a cipher. The digram frequencies are used to break two letter words such as an, to, of etc and the trigram frequencies are used to break three letter words such as the, are, for etc. After breaking a significant two letter and three letter words, it is practically east to identify the key from the cracked values of plaintext by matching the corresponding values in the ciphertext. This huge weakness in English language is used to break cipher texts encrypted using simple algorithms that make use of English alphabets. In practice the use of frequency analysis consists of first counting the frequency of ciphertext letters and then assigning "guessed" plaintext letters to them. "Many letters will occur with roughly the same frequency, so a cipher with X's may indeed map X onto R, but could also map X onto G or M. But some letters in every language using letters will occur more frequently; if there are more X's in the ciphertext than anything else, it's a good guess for English plaintext that X is a substitution for E. But T and A are also very common in English text, so X might be either of them also" [4]. Thus the cryptanalyst may need to try several combinations of mappings between ciphertext and plaintext letters. Once the common single letter frequencies have been resolved, then paired patterns and other patterns are solved. Finally, when sufficient characters have been cracked, then the rest of the text can be cracked using simple substitution. Frequency analysis is extremely effective against the simpler substitution ciphers and will break astonishingly short cipher texts with ease.

Attacks on Traditional algorithms
Encrypting using traditional algorithms have been defenseless against cryptanalytic attacks as they use bit by bit encryption, which can be easily broken using frequency analysis based attacks.

1. Caesar Cipher:
Considering the case of one of the oldest ciphers, the Caesar Cipher, this cipher replaces one letter of the plaintext with another to produce the ciphertext, and any particular letter in the plaintext will always, turn into the same letter in the cipher for all instance of the plaintext character. For instance, all B's will turn into F's. "Frequency analysis is based on the fact that certain letters, and combinations of letters, appear with characteristic frequency in essentially all texts in a particular language" [9]. For instance, in the English language, E is very common, while X is not. Likewise, ST, NG, TH, and QU are common combinations, while XT, NZ, and QJ are very uncommon, or even "impossible" to occur in English. This clearly shows how the Caesar cipher can be broken with ease by just identifying the frequency of each letter in the cipher text. A message encrypted using Caesar cipher is extremely insecure as an exhaustive cryptanalysis on the keys easily breaks the code.

2. Substitution Ciphers:
The Caesar cipher forms a subset of the entire set of substitution ciphers. Here, the key of the encryption process is the permutation of all the twenty six characters of the English alphabets. Rather than choosing a particular key for all encryption process, we use a different key for successive encryption processes. This technique increases the number of possible key to 26!, which is about 4 X 1026, which eliminates the exhaustive cryptanalysis attack on the keyspace [7]. To decrypt the cipher the, statistical frequency distribution of single letter occurrence in English language is analyzed. Then, the digram and trigram frequencies of standard English words are compared with the frequencies of the trigrams in the cipher to finally reconstruct the key and in turn decipher the text. This is an efficient method to break the substitution cipher as, each plaintext letter is represented by the same ciphertext letter in the message. So, all properties of plaintext are carried on to the cipher text.

3. Vigenere Cipher:
In a Vigenere cipher, there is greater security as, a given plaintext letter is not always represented by the same ciphertext letter. This is achieved by using a sequence of n different substitution ciphers to encrypt a message. This technique increases the possible number of keys from 26! to (26!)n. Although this was considered to be unbreakable, the Kasiski's method of attacking a Vigenere cipher yielded successful results of decrypting the message. According to this method, the first step is to find the key length (n).

Find identical segments of plain text that get encrypted to the same ciphertext, when they are b positions apart, where b=0 mod n. According to Kasiski, the next step is to find all the identical segments of length greater than 3, and record the distance between them [7].

This can then be used to predict the length of the key (n). Once this is found the key is found by an exhaustive search of the keyspace for all possible combinations to identify the key. This is done by substituting all possible values for n to generate substrings. Once the substring is formed, the plaintext message can be automatically identified by using the back substitution of the key into the cipher [7]. This can be done for all possible values for n until finally arriving at the actual key, which reveals the plaintext that was encrypted. This method can take a long time to break the key to identify the plaintext incase the key length is very long, as the keyspace value would be large for larger keys.

Defeating frequency based attacks:
Frequency based attacks have been used for a long time to break traditional encryption algorithms. It uses the fact that, traditional encryption algorithms do not eliminate the statistical properties of the language upon encryption.

The first way to defeat frequency based attacks is to encrypt blocks of characters at a time rather than single letters [7]. This would ensure that, the same text in the plaintext is not encrypted to the same text in the ciphertext upon encryption. For e.g., if we use the Caesar cipher encryption scheme, the word "ADDITIONAL" will be encrypted to "CFFKVKQPCN", we can see that the alphabets A, D and I are repeated more than once and at each instance, the encryption scheme used always encrypts A to C, D to F and I to K. This can clearly be used during frequency analysis to analyze the redundancy of the characters and in turn map them back to get the original plaintext character. Using a block encryption scheme, one can be satisfied that, this phenomenon does not occur as, in a block encryption scheme, the whole plaintext is broken into chunks or blocks of data, that is fed in as input to the encryption algorithm. The algorithm then, reads the input block along with the key and encrypts the complete block of plaintext, rather than individual characters, so there is a smaller chance that two blocks will produce the same chunk of ciphertext.

The second way of defeating frequency analysis is to make use of synonyms of words [7], rather than repeating the same word over and over again in a sentence. There are a lot of words in English, which have more than one synonym, thus providing with a set of words to be used as convenient in the particular context. To help in the selection of a synonym, grammar checking would have to be used to ensure that, the meaning expressed in the sentence is not altered by changing the words. Attacks against this technique could include creating a list of the best synonyms, but this would not help the attacker as different word could be used at each instance the same meaning needs to be expressed, defeating the benefit of this technique. This technique of using alternate words to represent common words to defeat cryptanalysis attacks is called "Homophones" [7] in cryptography.

A third technique that can effectively defeat cryptanalysis is Polyalphabetic substitution, that is, the use of "several alphabets to encrypt the message" [3], rather than using the same substitution technique again and again. The Vigenere Cipher is a form of Polyalphabetic cipher. This ensures that, no two characters are encrypted to the same ciphertext alphabet in the same message. This ensures that, direct frequency analysis of the cipher is not possible to successfully retrieve the original message. However, other techniques need to be used to identify the key length, if this is possible, then frequency analysis attack could be used to identify the original plaintext message successfully.

Finally, a possible technique that could be used to defeat frequency analysis is to "encrypt a single character of plaintext with two ciphertext characters" [3]. Upon encountering the same character twice, then different characters should be used to encrypt the message. This can be achieved by using a key size double that of the plaintext message and then encrypting the same plaintext with two values in the key and save them together for the same plaintext character. This would ensure that no two plaintext characters will have the same ciphertext character, defeating the frequency analysis method of breaking the cipher.

Modern encryption algorithms and cryptanalysis:
Modern cryptographic algorithms take a better approach in defeating frequency analysis based attacks. The cryptographic algorithms nowadays use block encryption, rather than encrypting characters bit by bit, thus eliminating the redundancy of ciphertext alphabets for similar plaintext alphabets. "Block ciphers are the central tool in the design of protocols for shared-key cryptography. A block cipher is a function E: {0, 1}k Ã- {0, 1}n ââ€ ' {0, 1}n. This notation means that E takes two inputs, one being a k-bit string and the other an n-bit string, and returns an n-bit string" [2]. The first input is the key, which is used to encrypt the secret message. The second string is called the plaintext, and the output is called a ciphertext. The key-length k and the block-length n are parameters associated to a specific block cipher. They vary from block cipher to block cipher, and depend on the design of the algorithm itself. Some of the most trusted symmetric ciphers include AES, Triple-DES, Blowfish, CAST and IDEA. In public-key cryptography, the most commonly used cryptosystems are RSA and the Diffie-Hellman systems, which have not been found to have any vulnerabilities till date.

Preferably, the block cipher E is a public specified algorithm. "In typical usage, a random key K is chosen and kept secret between a pair of users. The function EK is used by the sender to encrypt the message, for a given key, before sending it to the intended receiver, who decrypts the message using the same key" [2]. Security relies on the secrecy of the key. So, at first, one might think of the cryptanalyst's goal as recovering the key K given some ciphertext, intercepted during transmission. The block cipher should be designed to make this task computationally difficult. In order to achieve this, the algorithms that are used to encrypt the message must be designed with a high degree of mathematical complexity, which cannot be reversed to obtain the plaintext from a known ciphertext.

The length of the key used during encryption of a message plays an important role in deciding the effectiveness of an algorithm. Key length is conventionally measured in bits, and most of the well known strong ciphers have key lengths between 128 and 256 bits. A cipher is considered strong if, after years of attempts to find a weakness in the algorithm, there is no known effective cryptanalytic attack against it. This indicates that, the most efficient way of breaking an encrypted message without knowing the key used to encrypt it is to "brute force" it, i.e. trying all possible keys. "The effort required to break an encrypted message is determined by the number of possible keys, known as thekeyspace. Knowing the speed of the computer to break the key, it is easy to calculate how long it would take to search the keyspace to break a particular cipher" [2].

For example, considering a cipher that uses 128-bit keys, each bit can either be 0 or 1, so, there are 2128 or 3Ã-1038 keys approximately. Suppose we imagine that about ten billion computers are assigned the task of breaking the code, each capable of testing ten billion keys per second, then, the task of running through the entire keyspace would take around 3Ã-1018seconds, which is about 100 billion years. "But, in fact, it would be necessary to run through only half the keyspace to hit upon the correct key, which would take around 50 billion years. This is longer than the estimated age of the universe according to modern cosmology, which is about 15 billion years" [2]. This shows that, it is practically infeasible to crack modern cryptographic algorithms using Brute Force attacks. So, one can imagine the effectiveness of the modern cryptographic algorithms and their resistance towards cryptanalytic attacks.

Conclusions:
Cryptography has progressed in recent years and modern cryptographic algorithms have proved to be successful in defending against most forms of cryptanalytic attacks. Frequency analysis based attacks have proved to exploit the weaknesses in traditional encryption algorithms into revealing the plaintext message that was encrypted using them. The natural language used to encrypt messages is not considered to be random in nature, which is exploited by frequency counting based attacks. Based upon the frequency of letters that occur in the ciphertext, one can guess the plaintext characters due to their redundancy rate and the specific combination of letters in a word. This weakness can be repelled by using stream ciphers, which do not carry the redundancy in the plaintext to the ciphertext. Modern block cipher, encrypt a chunk of plaintext into ciphertext and vice versa, eliminating the redundancy of language used in encryption.

Although the algorithm plays an important part, it is the key length used in block ciphers that helps in repelling cryptanalysis. Modern ciphers use a key length starting from 128 bits, eliminating the possibility of a brute force attack to decrypt the message. The higher the key length, the more time it takes to break these ciphers. These advantages have made modern cryptographic algorithms more popular among the security community. No known weaknesses have been found in these algorithms yet, that may allow one to identify the plaintext message.

Learning a language is not a joke, especially a second or third one. I have had a rocky relationship with learning languages, starting from my native one: English. My second and third ones were Russian and Ukrainian. Russian sounded like an alien language to me and Ukrainian sounded like Slavic Italian. Either way, studying them has been a journey.

Most people who learn three languages to at least an intermediate level do not say they struggle with studying languages. But, that is my case. Even though I grew up in America, English was often most worst subject in school. My reading comprehension was bad, I had a speech impediment (though it has been resolved), and I was a late reader. It was not until my junior year of high school that I started to get decent grades in English classes. It was only through reading loads of books and writing tons of poems and stories that I finally began to grab hold of English, despite it being my native language. In other words, hard work and persistence were the only ingredients that solved my issue.

Next came Russian. I moved to Ukraine as an adventure and to live with my wife. It was difficult for her to move to the US, so I decided to relocate there instead. I came at a time when it was a blizzard, and that was a big shock to my system. I had been living in Seattle, Washington, were it hardly ever snowed. What was another surprise was learning the Russian language. My wife’s family is originally from Saint Petersburg, so naturally they spoke more Russian than Ukrainian. The first thought I had when studying Russian was that it sounded like an alien language to me. Even saying “hello” sounded strange and long. They say for the first year of studying a language, you are mostly listening and absorbing words, phrases, idioms, and the like. That is mostly my experience: I could barely put together a sentence after one year of listening to Russian every day and practicing a bit daily. I attended classes, but after only three months, I was not only discouraged with my progress, but I also did not want to bear anymore of the teacher’s yelling and intense homework. I was working full-time as an editor, and trying to finish my homework during my duties was too stressful. I had to remember whole pages of Russian text and recite it to my teacher. Although this method can be useful, it only made me want to study Russian less. Instead, I gave up these lessons and vied for more natural learning. I started to have long conversations with my mother-in-law and other people. I downloaded language-learning applications and the like to study on public transportation on my way to work. After five years of this, with some periods of leniency, I acquired a solid intermediate level in Russian, and could converse for long periods of time in Russian and understand about 80% of what people were saying around me. Like English, Russian took countless hours to become decent at. There was no trick or shortcut to become even adequate at it.

But eventually, I wanted to learn Ukrainian as well. I live in Ukraine for goodness’ sake. And, more importantly, in Kyiv, the city I reside in, people speak both Russian and Ukrainian. So, I was missing out on a large part of the culture. I soon found out that while Russian was interesting to me, I was passionate about learning Ukrainian. It sounds like Slavic Italian and has a much more pleasant culture attached to it, in my opinion. In fact, I got so enamored while studying Ukrainian that I forsake studying Russian simultaneously. I tried learning both at the same time, but my brain could not handle it. I felt like I was learning nothing in both. So, I have been sticking with Ukrainian for over two months now. Though I have noticed that after the second attempt, a third language is easier, the same problems arised for me: I was shy to speak the language, though I enjoyed reading, listening, and writing. As an editor, I feel the need to edit my speech as well. I always want to speak perfectly the first time. This is perhaps one of the biggest mistakes a language learner can make though. Perfection is the enemy of progress, they say. However, I have learned Ukrainian much faster than Russian due to my passion and interest in it.

People are motivated to do activities by many different factors. Each person is prompted by various influences to complete what they dream of fulfilling. For this paper, I will discuss what motivates me to finish what I set out to do. The main factors that are involved are competition, the desire for achieving greatness, the satisfaction of completing projects, and enjoyment.

Since I am a twin, a sense of competition is hardwired into my brain. Healthy competition can push us to do fantastic things. It makes us strive for a certain goal and gets us to do things faster. However, I do not think competition should become toxic. When anger, frustration, and depression is a result of competition, it is not healthy. Yet, competition can bring about respect, camaraderie, and mutual gain. In the case with my twin brother, we have pushed ourselves to be good at sports, music, school, and even at work. This competitive spirit has leaked into my life without my twin around. In a sense, I replicate my twin brother in other people. I always try to be number one at whatever I do—or at least in the top three. There are times this attitude becomes unhealthy, as I get frustrated if I give everything to a competition or activity and yield mediocre results. But as the years passed, I have gotten better at handling failure and defeat. All in all, I think I finish and do well in many areas of my life for the simple fact that I want to do well in them. Thriving in competition-based environments has made me do well in many tournaments and other events that engage in achieving top places. But, just the feeling of doing well against competent opponents is gratifying.

As I came from a childhood of low self-esteem due to bullying, medical problems, and stigmatization, achieving greatness has always been one of my goals. Like my goal of being in the top places of competitions, the yearning to achieve the heights of a discipline is a way for my self-esteem to get a boost. From a tender age, I wanted to be one of the greats in something: writing, music, sports. I just happen to be a writer now—so, I do my best to attain a high place among people of the written word. It is difficult to say, though, who will be labeled as “great” or “major” in the annals of history. Most famous writers die and then they became renowned worldwide. There are rare occasions when writers are famous in their lifetimes and much after. However, by the work I do, I hope that one day, my poems, essays, and stories will be in textbooks for school and I will be named as a “major poet” and such. Though I mostly write for the enjoyment of it, this concept of being remembered long after I have passed does have a potent motivating factor.

Besides wanting to be remembered and to show well at competitions, I also revel in the feeling of completing an important project. For instance, last year, I finished a poetry collection with my late father’s poems and my own poems. It was a tribute to him and his work. In a way, I felt that his death was not in vain and that his work was validated even more through this compilation. It is hard to describe the moment of knowing when such a prominent project is set to rest: it is almost like you are ready to die. You sense that a chapter of your life has closed and you are now a new person. These deep sentiments motivate me to complete projects on a regular basis.

The last, and most simple, factor that motivates me to do something is enjoyment. There are people who do activities they do not enjoy for decades. I am not one of those people. I can work on tasks I do not like for a while, but in the end, my main focus should be on something that I enjoy. If I find joy in doing a certain work, I engage in it for a large part of the day, or even all day without tiring. Tiredness in most cases, in my life, is due to a lack of enjoyment in the work being done.

Each person has his or her own motivating factors. For me, it happens to be competition, a desire for greatness, a want to complete projects, and enjoyment. I hope this reflection has allowed you to delve deeper into your true nature.

It is commonplace to say that all that was science fiction several decades ago has become reality nowadays. However, people often underestimate the extent to which space technologies have infiltrated our daily lives. We often use appliances or gadgets without even suspecting that they were initially developed to be used in space, in conditions of long-term orbital missions, and so on. Next time you watch a SpaceX launch on television or the Internet, you might want to look around yourself: chances are there is a bunch of things around you that can be referred to as “space technologies.”

Sometimes it can be difficult to believe that a mundane and boring item is of space origins. Take the famous “Dustbuster” as an example: a cordless compact vacuum cleaner massively sold in late 1970s. It was originally constructed for NASA’s Apollo space program. More specifically, during the program, there was a need for a self-contained, portable drill for gathering Moon rock samples. The drill was designed in such a way that it could provide solid motor power, while consuming a minimal amount of energy. The shape of the drill was the result of computer program calculations for optimal form-factor. Later, the idea of this battery-powered device was used as a basis for constructing a series of domestic appliances, of which the most famous one is the “Dustbuster” (Interesting Engineering).

NASA is connected not only to popular vacuum cleaners, but also to more sophisticated technologies we use daily. A computer mouse–something billions of people around the globe use–has appeared due to the bright minds working in the space agency. In particular, among the forefathers of the computer mouse were Bob Taylor and Doug Englebart. The former worked flight control systems and displays, as well as simulation technologies, and the latter was a researcher in the Stanford Research Institute. The joint effort of these two people led to a computer mouse being created in the 1960s. Back then, people mostly saw computers as powerful calculators capable of performing complicated arithmetic. They were bulky, they were inconvenient to use, and manipulating data on them was difficult. The “mouse project” started as a search for alternative ways of manipulating data gathered by NASA during space flights. After Englebart received funding for his project, the device he came up with–the mouse–quickly overcame its competing analogs (the most promising was the so-called light pen). Later, Englebart worked on other computer-related projects, among which was a network of computers the users of which could exchange and transfer information (NASA)—but that is a different story.

Tragedies in Japan and Nepal caused by enormous earthquakes showed how dangerous living in seismic areas can be. Not that it happened for the first time in history, but it definitely boosted the development of a number of technologies meant to protect people and buildings from such disasters. Currently, about 550 buildings and bridges around the world are using shock absorbers–constructions whose main goal is to mitigate earth tremors and shockwaves caused by them. In particular, such absorbers are used in Japan, Taiwan, and the United States. But where did the technology come from? Obviously, from space–or more specifically, from cosmodromes. When a spacecraft is being launched, it needs special suspenders that help it maintain vertical orientation. These suspenders must be capable of moving away quickly during a launch, otherwise they will interfere with the takeoff process and damage the spacecraft, or prevent it from flying. The problem is that suspenders are moved away with such force and velocity that they may overexert and collide with the spacecraft. In order to prevent that, shock absorbers similar to those used in buildings and bridges have been used. This technology has been around approximately since 1969, when humanity first launched a space vessel to the Moon, and has proved itself extremely useful multiple times (Marketplace). So, if you live in Tokyo, San Francisco, or any other seismically active place, chances are that you work or live in a building that utilizes shock absorbers.

Next time you take out your cell phone intending to take a selfie, think of NASA engineers standing behind mobile cameras in your Android or iOS device. Indeed, such cameras are yet another technology first developed for the needs of space travel. It is not like NASA scientists looked for ways to take better pictures of themselves. However, they do need high quality images, and they want their cameras to be miniature but stable and reliable. The research conducted by NASA engineer Eric Fossum was aimed at that: miniaturizing cameras and making them capable of withstanding the harsh conditions of outer space. His contribution was namely reinventing and enhancing CMOS (complementary metal oxide semiconductor) matrix technology, creating active pixel sensors. These sensors, unlike previous generations of CMOS technology (the attempts to implement it had been made since 1960s, but none succeeded because of technological shortcomings), can produce high quality images, and thus have been adapted to cell phones and other portable devices (Interesting Engineering).

This list of technologies originally invented within the space industry is not complete. However, it illustrates the fact that even the most mundane and/or habitual technologies are not as simple as they might appear.

